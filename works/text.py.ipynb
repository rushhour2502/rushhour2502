# with open (r"C:\Windows\System32\energy-report.html", "r", encoding="UTF-8") as file:
#     soup = BeautifulSoup(file, "html.parser")
#     x = soup.prettify()
#     print(x)


# Open the HTML file
# with open(r"C:\Windows\System32\energy-report.html", "r", encoding="utf-8") as file:
#     soup = BeautifulSoup(file, 'html.parser')

import requests
from bs4 import BeautifulSoup

# url = "https://en.wikipedia.org/wiki/Pharmacy"
# response = requests.get(url).content
# soup = BeautifulSoup(response, "html.parser")


# # Extract table data (details about the system)
# table = soup.find('table')
# rows = table.find_all('tr')

# # Create a dictionary to store the details
# details = {}

# # Extract key-value pairs from the table rows
# for row in rows:
#     cols = row.find_all('td')
#     if len(cols) == 2:
#         key = cols[0].text.strip()
#         value = cols[1].text.strip()
#         details[key] = value

# # Print system details in a clean format
# for key, value in details.items():
#     print(f"{key}: {value}")

# # Extract the Errors section
# errors_section = soup.find('h4', text="Errors")
# error_logs = errors_section.find_next('div', class_="error-log-entry")

# # Clean and display the error logs
# error_header = error_logs.find('div', class_="log-entry-header").text.strip()
# error_description = error_logs.find('div', class_="log-entry-description").text.strip()
# error_table = error_logs.find('table').find_all('tr')

# # Display the error details
# print(f"\nError: {error_header}")
# print(f"Description: {error_description}")

# # Display error device details
# for row in error_table:
#     cols = row.find_all('td')
#     if len(cols) == 2:
#         key = cols[0].text.strip()
#         value = cols[1].text.strip()
#         print(f"{key}: {value}")

import requests
from bs4 import BeautifulSoup

# Fetch the HTML content from the URL
url = "https://en.wikipedia.org/wiki/Pharmacy"
response = requests.get(url)

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")

# Find the main content of the page
main_content = soup.find(id="bodyContent")

# Initialize a list to hold the cleaned text
cleaned_text = []

if main_content:
    # Extract and process each section of the content
    for element in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
        if element.name == 'p':
            cleaned_text.append(element.get_text(strip=True))
        elif element.name.startswith('h'):
            cleaned_text.append(f"\n{element.get_text(strip=True)}\n")

    # Join all parts with a single new line and print
    print("\n".join(cleaned_text))
else:
    print("Main content not found.")
